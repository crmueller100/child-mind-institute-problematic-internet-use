{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext cudf.pandas\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import seaborn as sns\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os\n",
    "    \n",
    "from tqdm import tqdm\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score # To calculate quadratic weighted Kappa\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone, BaseEstimator, RegressorMixin\n",
    "\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "\n",
    "from scipy.stats import mode\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "running_locally = True\n",
    "rerun_series = False\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if running_locally:\n",
    "    train_df = pd.read_csv('train.csv')\n",
    "    test_df = pd.read_csv('test.csv')\n",
    "    data_dictionary = pd.read_csv('data_dictionary.csv')\n",
    "else:\n",
    "    train_df = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "    test_df = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "    data_dictionary = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/data_dictionary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Severity Impairment Index\n",
    "First, we'll look at the distribution of `sii` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sii_data = train_df['sii'].value_counts().reset_index()\n",
    "sii_data.columns = ['sii_label', 'count']\n",
    "sii_data['percent_of_total'] = round((sii_data['count'] / len(train_df)) * 100,1)\n",
    "sii_data\n",
    "print(f\"{sii_data}\\n\")\n",
    "print(f\"{100.0*train_df['sii'].isnull().sum()/len(train_df):.2f}% of the sii data is null\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographics\n",
    "Let's look through the data related to demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Basic_Demos-Sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8')\n",
    "sns.boxplot(x='Basic_Demos-Age', y='sii', data=train_df)\n",
    "plt.title('SII by Enrollment Season')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_sii_counts = train_df.groupby(['Basic_Demos-Age', 'sii']).size().reset_index(name='count')\n",
    "sns.barplot(data=age_sii_counts, x='Basic_Demos-Age', y='count', hue='sii')\n",
    "plt.title('Counts of Basic_Demos-Age by sii Category')\n",
    "plt.xlabel('Basic_Demos-Age')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='sii')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what we learned:\n",
    "* We learned that our dataset is mostly males, about 63% male to 37% female\n",
    "* Also, the interquartile range of `sii` increases with older ages. \n",
    "* Younger ages are prone to less severe impairment\n",
    "* The magnitude of the severity changes based on the age in consideration. E.g.\n",
    "    * 0 risk -> 8 yrs old\n",
    "    * 1 risk -> 10 yrs old\n",
    "    * 2 risk -> 12 yrs old\n",
    "    * 3 risk -> 13 yrs old\n",
    "\n",
    "Not shown:\n",
    "* No trend between `Basic_Demos-Enroll_Season` and `sii`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Children's Global Assessment Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data = train_df.groupby(['CGAS-CGAS_Score', 'CGAS-Season'])['sii'].mean().reset_index()\n",
    "heatmap_data = heatmap_data.pivot(index='CGAS-CGAS_Score', columns='CGAS-Season', values='sii')\n",
    "column_order = [\"Spring\", \"Summer\", \"Fall\", \"Winter\"]\n",
    "heatmap_data = heatmap_data.reindex(columns=column_order) # want the seasons to be in chronological\n",
    "sns.heatmap(heatmap_data, annot=True, cmap='viridis', fmt=\".2f\") \n",
    "plt.title('Average Problematic Internet Use (sii) by CGAS Score and Season')\n",
    "plt.xlabel('CGAS Season')\n",
    "plt.ylabel('CGAS Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No notable trends seen in the Children's Global Assessment Scale. Let's move on to the next instrument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physical Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phys_cols = list(train_df.filter(like='Physical-').columns) + ['sii']\n",
    "train_df[phys_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='sii', y='Physical-BMI', data=train_df)\n",
    "plt.title('BMI vs. Problematic Internet Use')\n",
    "plt.xlabel('Sii')\n",
    "plt.ylabel('BMI (kg/m^2)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3)\n",
    "sns.histplot(train_df[train_df['Basic_Demos-Sex'] == 0]['Physical-Height'], kde=True, ax=axes[0])\n",
    "axes[0].set_title('Height Distribution (Male)')\n",
    "\n",
    "sns.histplot(train_df[train_df['Basic_Demos-Sex'] == 1]['Physical-Height'], kde=True, ax=axes[1])\n",
    "axes[1].set_title('Height Distribution (Female)')\n",
    "\n",
    "sns.histplot(train_df['Physical-Height'], kde=True, ax=axes[2])\n",
    "axes[2].set_title('Height Distribution (All)')\n",
    "\n",
    "plt.suptitle('Height Distributions')\n",
    "plt.xlabel('Height (inches)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what we learned: \n",
    "- Higher BMI corresponds to higher `sii`. The 50 percentile of BMI for \"Severe\" sii is higher than the 75 percentile for \"Moderate\" sii\n",
    "- There was minimal variance in the season w.r.t. the `sii`\n",
    "- The distribution of heights seemed to take on the form of a bimodal distribution instead of a typical normal distribution. This is likely because males and females are different heights, so the resulting histogram has 2 peaks. The males are the shorter peak because they are typically shorter in the age range collected in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FitnessGram Vitals and Treadmill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness_endurance_cols = list(train_df.filter(like='Fitness_Endurance-').columns) + ['sii']\n",
    "train_df[fitness_endurance_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcode the columns to avoid the category columns\n",
    "train_df[['Fitness_Endurance-Max_Stage','Fitness_Endurance-Time_Mins','Fitness_Endurance-Time_Sec','sii']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(train_df, x='Fitness_Endurance-Time_Mins',bins=20, hue='sii')\n",
    "plt.title('Fitness Endurance Time vs. Problematic Internet Use')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Fitness_Endurance-Time_Mins')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what we learned:\n",
    "- `Fitness_Endurance-Time_Mins` and `Fitness_Endurance-Time_Sec` should be combined into 1 column. This will be done later\n",
    "- There wasnt a strong correlation with any of these columns and `sii`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FitnessGram Child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgc_cols = list(train_df.filter(like='FGC').columns) + ['sii']\n",
    "train_df[fgc_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = train_df[fgc_cols].drop(columns=['FGC-Season']).corr()\n",
    "sorted_cols = corr_matrix.columns.sort_values()\n",
    "sorted_corr_matrix = corr_matrix.loc[sorted_cols, sorted_cols]\n",
    "\n",
    "sns.heatmap(sorted_corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what we learned:\n",
    "- Interestingly, there isn't as high of a correlation between the raw values and the categorized \"zones\". \n",
    "    - The highest observed correlation between a category and its zone was only 0.75 for `FGC_FGC_SRR` and `FGC_FGC_SRL`. (These two measurements are just the left and right legs of the Sit & Reach.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bio-electric Impedance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bia_cols = list(train_df.filter(like='BIA-').columns) + ['sii']\n",
    "train_df[bia_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bia_corr_matrix = train_df[bia_cols].drop(columns=['BIA-Season']).corr()\n",
    "bia_sorted_cols = bia_corr_matrix.columns.sort_values()\n",
    "bia_sorted_corr_matrix = bia_corr_matrix.loc[bia_sorted_cols, bia_sorted_cols]\n",
    "\n",
    "sns.heatmap(bia_sorted_corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that there are some highly correlated features. Let's explore that a bit more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "correlated_bia_features = ['BIA-BIA_BMR','BIA-BIA_DEE','BIA-BIA_ECW','BIA-BIA_FFM','BIA-BIA_Fat','BIA-BIA_ICW','BIA-BIA_LDM','BIA-BIA_LST','BIA-BIA_SMM','BIA-BIA_TBW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: uncomment this later\n",
    "# pairplot_features = correlated_bia_features + ['sii']\n",
    "# sns.pairplot(train_df[pairplot_features], hue='sii', palette='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what we learned:\n",
    "- Body Fat Percentage is inversely correlated to most BIA fields, i.e. Bone Mineral Content, Basal Metabolic Rate, Daily Energy Expenditure, Extracellular Water, Fat Free Mass, Intracellular Water, Lean Dry Mass, Lean Soft Tissue, Skeletal Muscle Mass, Total Body Water.\n",
    "- Basal Metabolic Rate, Daily Energy Expenditure, Extracellular Water, and Fat Free Mass are all correlated to each other.\n",
    "- Similarly, Intracellular Water, Lean Dry Mass, Lean Soft Tissue, Skeletal Muscle Mass, and Total Body Water are correlated to each other\n",
    "- No one attribute correlates to `sii`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physical Activity Questionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2)\n",
    "sns.barplot(data=train_df, x='PAQ_A-Season', y='PAQ_A-PAQ_A_Total', hue='sii', ax=axes[0])\n",
    "axes[0].set_title('Physical Activity Questionnaire (Adults)')\n",
    "\n",
    "sns.barplot(data=train_df, x='PAQ_C-Season', y='PAQ_C-PAQ_C_Total', hue='sii', ax=axes[1])\n",
    "axes[1].set_title('Physical Activity Questionnaire (Children)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what we learned:\n",
    "- There are no adults with a \"severe\" `sii` rating in the Summer.\n",
    "- Winter was the season with the most \"severe\" `sii` ratings for children, but it was the season with the fewest \"severe\" `sii` ratings for adults. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parent-Child Internet Addiction Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pciat_aggs = train_df.groupby('sii')['PCIAT-PCIAT_Total'].agg(['min', 'max', 'mean'])\n",
    "pciat_aggs = pciat_aggs.rename(\n",
    "    columns={'min': 'Minimum PCIAT total Score', 'max': 'Maximum total PCIAT Score', 'mean': 'Average total PCIAT Score'}\n",
    ")\n",
    "pciat_aggs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all columns that are of the form PCIAT-PCIAT_XX\n",
    "pciat_columns = [f'PCIAT-PCIAT_{i:02d}' for i in range(1, 21)]\n",
    "\n",
    "pciat_summed_total = train_df[pciat_columns].fillna(0).sum(axis=1)\n",
    "is_calculated_sum_equal_to_total_column = pciat_summed_total == train_df['PCIAT-PCIAT_Total'].fillna(0)\n",
    "\n",
    "# Check if sum of the scores equals total for all rows\n",
    "is_calculated_sum_equal_to_total_column.sum() == len(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what we learned:\n",
    "- The average of the scores for each `sii` classification was around the median of the bounds of the range (e.g. n `sii` of 0 is from 0-30, and the average score was ~14)\n",
    "- The sum of the PCIAT scores all align with the classification set by the Severity Impairment Index (sii).\n",
    "- The number of instances where PCIAT-PCIAT_1 through PCIAT-PCIAT_20 is equal to PCIAT-PCIAT_Total is equal to the number of records in the dataset. This shows that PCIAT-PCIAT_Total is 100% is a linear transformation of other features in this dataset, so we can drop it from the dataset later. However, many of the columns are `nan`. This means that if an entry's sum is 30 but, for instance, there are 10 `nan` values, that record is likely misclassified. We will account for this in the Feature Engineering section.\n",
    "\n",
    "Also, note that the **test** set does not have any of the PCIAT data, and we're missing about 30% of the `sii` label in the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sleep Disturbance Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_cols = list(train_df.filter(like='SDS-').columns) + ['sii']\n",
    "train_df[sds_cols].drop(columns=['sii']).isnull().sum()/train_df[['SDS-Season','SDS-SDS_Total_Raw','SDS-SDS_Total_T']].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcode to omit the category columns\n",
    "train_df[['SDS-SDS_Total_Raw','SDS-SDS_Total_T']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what we learned:\n",
    "- About half of the sleep data is null\n",
    "- There is an extremely strong correlation between the Total Raw Score and Total T-score. We will remove one in the Feature Engineering Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internet Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_use = list(train_df.filter(like='PreInt_EduHx-').columns) + ['sii']\n",
    "train_df[int_use].drop(columns='PreInt_EduHx-Season').corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what we learned:\n",
    "- There is a weak correlation between Hours of Using Computer/Internet and `sii`. This is the strongest correlation we have seen so far between a raw column and `sii`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rerun_series:\n",
    "    def process_file(filename, dirname):\n",
    "        data = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "        data.drop('step', axis=1, inplace=True)\n",
    "        return data.describe().values.reshape(-1), filename.split('=')[1]\n",
    "\n",
    "    def load_time_series(dirname) -> pd.DataFrame:\n",
    "        ids = [d for d in os.listdir(dirname) if os.path.isdir(os.path.join(dirname, d))]\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n",
    "        stats, indexes = zip(*results)\n",
    "\n",
    "        data = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n",
    "        data['id'] = indexes\n",
    "        return data\n",
    "\n",
    "    if running_locally:\n",
    "        train_ts = load_time_series('./series_train.parquet')\n",
    "        test_ts = load_time_series('./series_test.parquet')\n",
    "        random_raw_parquet_file = pd.read_parquet('series_train.parquet/id=0417c91e/part-0.parquet')\n",
    "    else:\n",
    "        train_ts = load_time_series('/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet')\n",
    "        test_ts = load_time_series('/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet')\n",
    "\n",
    "    train_ts.to_csv('train_ts.csv', index=False)\n",
    "    test_ts.to_csv('test_ts.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not rerun_series:\n",
    "    train_ts = pd.read_csv('train_ts.csv')\n",
    "    test_ts = pd.read_csv('test_ts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine `Fitness_Endurance-Time_Mins` and `Fitness_Endurance-Time_Sec` into 1 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_fe_time_into_one_column(df):\n",
    "    # check if either column is null\n",
    "    null_mask = df['Fitness_Endurance-Time_Mins'].isnull() | df['Fitness_Endurance-Time_Sec'].isnull()\n",
    "    df['Fitness_Endurance-Time'] = df['Fitness_Endurance-Time_Mins'] + df['Fitness_Endurance-Time_Sec'] / 60\n",
    "\n",
    "    # set result to null if either column is null\n",
    "    df.loc[null_mask, 'Fitness_Endurance-Time'] = np.nan  \n",
    "\n",
    "    df.drop(columns=['Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec'], inplace=True)\n",
    "\n",
    "combine_fe_time_into_one_column(test_df)\n",
    "combine_fe_time_into_one_column(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SDS-SDS_Total_Raw` and `SDS-SDS_Total_T` have a 0.996 correlation. Remove the one smaller effect on the target variable. We can use a Random Forest for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state=42)\n",
    "temp_df = train_df[train_df['sii'].notnull()].copy()\n",
    "temp_df.dropna(subset=['SDS-SDS_Total_Raw', 'SDS-SDS_Total_T'], inplace=True)\n",
    "model.fit(temp_df[['SDS-SDS_Total_Raw', 'SDS-SDS_Total_T']], temp_df['sii'])\n",
    "\n",
    "print(*zip(model.feature_importances_, model.feature_names_in_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SDS-SDS_Total_T` has a smaller effect on the target variable, so we will drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sds_column(df):\n",
    "    if 'SDS-SDS_Total_T' in df.columns:\n",
    "        df.drop(columns=['SDS-SDS_Total_T'], inplace=True)\n",
    "remove_sds_column(train_df)\n",
    "remove_sds_column(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCIAT\n",
    "There is a data point with no PCIAT scores but a total value of 0. It has an `PreInt_EduHx-computerinternet_hoursday` of 2, but an `sii` of 0. If over 70% of the PCIAT scores are null, then we cannot rely on the `PCIAT-PCIAT_Total` score, so we null it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pciat_component_cols = [f'PCIAT-PCIAT_{i:02}' for i in range(1, 21)]\n",
    "null_percentage = train_df[pciat_component_cols].isnull().mean(axis=1)\n",
    "\n",
    "mask = null_percentage > 0.7  # Can adjust this to make it more or less strict\n",
    "train_df.loc[mask, 'PCIAT-PCIAT_Total'] = np.nan\n",
    "# No PCIAT columns on test_df, so don't need to transform it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a record is missing multiple values in the PCIAT evaluations, it could potentially be missing data that would **increase the severity** of the `sii`. The dataset treats those values as 0, but that is not a fair assumption. If there are a sufficient values from a row missing, we should impute the missing values using the average value for that column and re-calculate the `PCIAT-PCIAT_TOTAL` (and subsequently, the `sii`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only operate on rows where the score is already >= 10. With only 20 columns, getting to 30 points to move up in severity won't happen.\n",
    "temp_pciat_df = train_df[(train_df['PCIAT-PCIAT_Total'] >= 10) & (train_df[pciat_component_cols].isnull().any(axis=1))].copy()\n",
    "\n",
    "temp_pciat_df_filled = temp_pciat_df[pciat_component_cols].apply(lambda x: x.fillna(x.mean()), axis=0)\n",
    "temp_pciat_df['PCIAT_Sum_Filled'] = temp_pciat_df_filled.sum(axis=1)\n",
    "\n",
    "train_df.loc[temp_pciat_df.index, 'PCIAT_Sum_Filled'] = temp_pciat_df['PCIAT_Sum_Filled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_bucket(value):\n",
    "    if 0 <= value <= 30:\n",
    "        return 0\n",
    "    elif 31 <= value <= 49:\n",
    "        return 1\n",
    "    elif 50 <= value <= 79:\n",
    "        return 2\n",
    "    elif 80 <= value:\n",
    "        return 3\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Apply the bucket assignment function to both columns\n",
    "train_df['PCIAT_Sum_Classify_Sii'] = train_df['PCIAT_Sum_Filled'].apply(assign_bucket)\n",
    "\n",
    "rows_to_be_updated = train_df[train_df['PCIAT_Sum_Classify_Sii'] > train_df['sii']]\n",
    "print(rows_to_be_updated[['sii', 'PCIAT_Sum_Classify_Sii']])\n",
    "\n",
    "train_df['sii'] = np.where(\n",
    "    train_df['PCIAT_Sum_Classify_Sii'] > train_df['sii'],  # Condition\n",
    "    train_df['PCIAT_Sum_Classify_Sii'],                    # New value if condition is met\n",
    "    train_df['sii']                                        # Keep existing value otherwise\n",
    ")\n",
    "\n",
    "train_df.drop(columns=['PCIAT_Sum_Filled', 'PCIAT_Sum_Classify_Sii'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also check for outliers and replace them with `NaN`. As an example, `CGAS-CGAS_Score` has a value of 999, which is an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outliers(df, column, valid_min, valid_max, placeholder_value=np.nan):\n",
    "    outliers = (df[column] < valid_min) | (df[column] > valid_max)\n",
    "    \n",
    "    print(f\"Found {outliers.sum()} outliers in column '{column}'.\")\n",
    "    \n",
    "    df.loc[outliers, column] = placeholder_value\n",
    "\n",
    "handle_outliers(train_df, 'CGAS-CGAS_Score', 0, 100, np.nan)\n",
    "handle_outliers(test_df, 'CGAS-CGAS_Score', 0, 100, np.nan)\n",
    "handle_outliers(train_df, 'SDS-SDS_Total_Raw', 0, 100, np.nan)\n",
    "handle_outliers(test_df, 'SDS-SDS_Total_Raw', 0, 100, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Sparse Records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a handful of records that are almost entirely null. We will remove these from the training data. However, every record has a value for`id`, `Basic_Demos-Age`, and `Basic_Demos-Sex`. Therefore, if a record has all remaining columns listes as `NaN`, it is safe to drop them from `train_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "print(f\"There are {len(train_df.columns)} columns in the training data and {len(test_df.columns)} columns in the test data.\\n\")\n",
    "null_counts_per_row = train_df.isnull().sum(axis=1)\n",
    "null_counts_distribution = null_counts_per_row.value_counts().sort_index()\n",
    "print(f\"Here are the number of null columns each record has:\\n {null_counts_distribution}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_mostly_null_rows(df):\n",
    "    columns_to_check = [col for col in df.columns if 'stat' not in col]\n",
    "    threshold = 0.10  # require at least 10% of the dataset be non-null return df\n",
    "    row_threshold = int(threshold * (len(columns_to_check)))\n",
    "    df.dropna(thresh=row_threshold, subset=columns_to_check, axis=0, inplace=True)\n",
    "    return df\n",
    "\n",
    "train_df = drop_mostly_null_rows(train_df)\n",
    "test_df = drop_mostly_null_rows(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Feature Interaction Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can try this. Tests all the combos. Beware the curse of dimensionality\n",
    "# from itertools import combinations\n",
    "\n",
    "# columns = correlated_bia_features\n",
    "# feature_pairs = list(combinations(columns, 2))\n",
    "# def create_interaction_features(df):\n",
    "#     new_features = []\n",
    "#     for f1, f2 in feature_pairs:\n",
    "#         new_df = pd.DataFrame()\n",
    "#         new_df[f'{f1}_x_{f2}'] = df[f1] * df[f2]\n",
    "#         new_df[f'{f1}_div_{f2}'] = df[f1] / (df[f2] + 1e-5)\n",
    "#         new_features.append(new_df)\n",
    "\n",
    "#     df = pd.concat([df] + new_features, axis=1)\n",
    "#     return df\n",
    "\n",
    "# # Apply the function to both train_df and test_df\n",
    "# train_df = create_interaction_features(train_df)\n",
    "# test_df = create_interaction_features(test_df)\n",
    "\n",
    "\n",
    "def feature_engineering(df):\n",
    "    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n",
    "    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n",
    "    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n",
    "    df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n",
    "    df['FFMI_BFP'] = df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat']\n",
    "    df['FMI_BFP'] = df['BIA-BIA_FMI'] / df['BIA-BIA_Fat']\n",
    "    df['LST_TBW'] = df['BIA-BIA_LST'] / df['BIA-BIA_TBW']\n",
    "    df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n",
    "    df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE']\n",
    "    # df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n",
    "    # df['DEE_Weight'] = df['BIA-BIA_DEE'] / df['Physical-Weight']\n",
    "    df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n",
    "    df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n",
    "    # df['Hydration_Status'] = df['BIA-BIA_TBW'] / df['Physical-Weight']\n",
    "    df['ICW_TBW'] = df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n",
    "    df['BMI_PHR'] = df['Physical-BMI'] * df['Physical-HeartRate']\n",
    "    return df\n",
    "\n",
    "train_df = feature_engineering(train_df)\n",
    "test_df = feature_engineering(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use k-NN to Impute Missing Numeric, Non-categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_numeric_and_non_categorical_features_from_df(df):\n",
    "    numeric_columns = df.select_dtypes(include=['int32', 'int64', 'float64']).columns\n",
    "    dd_fields = data_dictionary[data_dictionary['Type'] == 'categorical int']['Field'].tolist()\n",
    "\n",
    "    # Only normalize numeric columns that are NOT categorical int\n",
    "    features = [col for col in numeric_columns if col not in dd_fields and col != 'sii']\n",
    "    return features\n",
    "# features_to_normalize = get_features_numeric_and_non_categorical_features_from_df(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_impute(df, n_neighbors=5):\n",
    "    features_to_impute = get_features_numeric_and_non_categorical_features_from_df(df)\n",
    "    \n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "\n",
    "    # Perform k-NN imputation and ensure the result integrates with the DataFrame\n",
    "    imputed_data = imputer.fit_transform(df[features_to_impute])\n",
    "    df[features_to_impute] = pd.DataFrame(imputed_data, columns=features_to_impute, index=df.index)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = knn_impute(train_df, n_neighbors=5)\n",
    "test_df = knn_impute(test_df, n_neighbors=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use k-NN to Impute Missing Numeric, Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic_Demos-Sex was removed from this list\n",
    "numeric_categorical_data = ['FGC-FGC_CU_Zone','FGC-FGC_GSND_Zone','FGC-FGC_GSD_Zone','FGC-FGC_PU_Zone','FGC-FGC_SRL_Zone','FGC-FGC_SRR_Zone','FGC-FGC_TL_Zone','BIA-BIA_Activity_Level_num','BIA-BIA_Frame_num','PCIAT-PCIAT_01','PCIAT-PCIAT_02','PCIAT-PCIAT_03','PCIAT-PCIAT_04','PCIAT-PCIAT_05','PCIAT-PCIAT_06','PCIAT-PCIAT_07','PCIAT-PCIAT_08','PCIAT-PCIAT_09','PCIAT-PCIAT_10','PCIAT-PCIAT_11','PCIAT-PCIAT_12','PCIAT-PCIAT_13','PCIAT-PCIAT_14','PCIAT-PCIAT_15','PCIAT-PCIAT_16','PCIAT-PCIAT_17','PCIAT-PCIAT_18','PCIAT-PCIAT_19','PCIAT-PCIAT_20','PreInt_EduHx-computerinternet_hoursday']\n",
    "\n",
    "def knn_impute_categorical(df, categorical_columns, n_neighbors=5):\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "\n",
    "    # Need to check this because test_df doesn't have all the columns as train_df\n",
    "    valid_columns = [col for col in categorical_columns if col in df.columns]\n",
    "\n",
    "    # Fit-transform on valid columns\n",
    "    imputed_data = imputer.fit_transform(df[valid_columns])\n",
    "    df[valid_columns] = pd.DataFrame(imputed_data, columns=valid_columns, index=df.index)\n",
    "\n",
    "    df[valid_columns] = df[valid_columns].round().astype(int)\n",
    "    return df\n",
    "\n",
    "train_df = knn_impute_categorical(train_df, numeric_categorical_data)\n",
    "test_df = knn_impute_categorical(test_df, numeric_categorical_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actigraphy Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Was getting a kernel error for a bit. Thought you could set the device to 'mps' on Mac, but it didn't work. Set to 'cpu' and it worked, so I left it\n",
    "mps_device = torch.device(\"mps\")\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*3, encoding_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*2, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, input_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim*2, input_dim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim*3, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "def perform_autoencoder(df, encoding_dim=50, epochs=50, batch_size=32):\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df)\n",
    "    \n",
    "    data_tensor = torch.FloatTensor(df_scaled, device='cpu')\n",
    "    \n",
    "    input_dim = data_tensor.shape[1]\n",
    "    autoencoder = AutoEncoder(input_dim, encoding_dim).to('cpu')\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(data_tensor), batch_size):\n",
    "            batch = data_tensor[i : i + batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = autoencoder(batch)\n",
    "            loss = criterion(reconstructed, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}]')\n",
    "                 \n",
    "    with torch.no_grad():\n",
    "        encoded_data = autoencoder.encoder(data_tensor).numpy()\n",
    "        \n",
    "    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i + 1}' for i in range(encoded_data.shape[1])])\n",
    "    \n",
    "    return df_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ts_no_id = train_ts.drop('id', axis=1)\n",
    "test_ts_no_id = test_ts.drop('id', axis=1)\n",
    "\n",
    "if False or not running_locally:\n",
    "    train_ts_encoded = perform_autoencoder(train_ts_no_id, encoding_dim=50, epochs=100, batch_size=32)\n",
    "    test_ts_encoded = perform_autoencoder(test_ts_no_id, encoding_dim=50, epochs=100, batch_size=32)\n",
    "\n",
    "    train_ts_encoded.to_csv('train_ts_encoded.csv', index=False)\n",
    "    test_ts_encoded.to_csv('test_ts_encoded.csv', index=False)\n",
    "\n",
    "else:\n",
    "    train_ts_encoded = pd.read_csv('train_ts_encoded.csv')\n",
    "    test_ts_encoded = pd.read_csv('test_ts_encoded.csv')\n",
    "\n",
    "train_ts_encoded[\"id\"] = train_ts[\"id\"]\n",
    "test_ts_encoded['id'] = test_ts[\"id\"]\n",
    "\n",
    "train_df = pd.merge(train_df, train_ts_encoded, how=\"left\", on='id')\n",
    "test_df = pd.merge(test_df, test_ts_encoded, how=\"left\", on='id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will define quadratic weighted kappa as our error function and create our train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_qwk(y1, y2):\n",
    "    return cohen_kappa_score(y1, y2, weights='quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can't use imputation to predict the value for `sii`, so we drop those that are null\n",
    "train_df.dropna(subset=['sii'], inplace=True)\n",
    "\n",
    "X = train_df.drop(columns=['sii','id'])\n",
    "y = train_df['sii']\n",
    "# Need to drop all the PCIAT-PCIAT_# columns because they are not given in the test data\n",
    "X = X.drop(columns=[col for col in X.columns if 'PCIAT' in col])\n",
    "\n",
    "test_df_ids = test_df['id']\n",
    "test_df = test_df.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define one-hot encoding of seasonal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_columns = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n",
    "          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n",
    "          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n",
    "def one_hot_encode_seasons(train_df, test_df):    \n",
    "    for col in season_columns:\n",
    "        \n",
    "        if col in train_df.columns and col in test_df.columns:\n",
    "            all_categories = sorted(pd.concat([train_df[col], test_df[col]], axis=0).dropna().unique())\n",
    "            \n",
    "            train_dummies = pd.get_dummies(train_df[col], prefix=col).reindex(columns=[f\"{col}_{cat}\" for cat in all_categories], fill_value=0)\n",
    "            test_dummies = pd.get_dummies(test_df[col], prefix=col).reindex(columns=[f\"{col}_{cat}\" for cat in all_categories], fill_value=0)\n",
    "\n",
    "            train_df = pd.concat([train_df, train_dummies], axis=1)\n",
    "            test_df = pd.concat([test_df, test_dummies], axis=1)\n",
    "\n",
    "            # Drop the original column\n",
    "            train_df.drop(col, axis=1, inplace=True)\n",
    "            test_df.drop(col, axis=1, inplace=True)\n",
    "    return train_df, test_df\n",
    "\n",
    "def categorical_encode_seasons(train_df, test_df):\n",
    "    for col in season_columns:\n",
    "        if col in train_df.columns and col in test_df.columns:\n",
    "            train_df[col] = train_df[col].fillna('Missing')\n",
    "            test_df[col] = test_df[col].fillna('Missing')\n",
    "            train_df[col] = train_df[col].astype('category')\n",
    "            test_df[col] = test_df[col].astype('category')\n",
    "\n",
    "            le = LabelEncoder() # use same encoder for both train and test\n",
    "            train_df[col] = le.fit_transform(train_df[col])\n",
    "            test_df[col] = le.transform(test_df[col])\n",
    "\n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_rounder(oof_non_rounded, thresholds):\n",
    "    \"\"\"\n",
    "    Rounds predictions to the nearest class based on thresholds.\n",
    "    \"\"\"\n",
    "    output = np.zeros_like(oof_non_rounded, dtype=int)\n",
    "    for i, threshold in enumerate(thresholds):\n",
    "        output[oof_non_rounded >= threshold] = i + 1\n",
    "    return output\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    \"\"\"\n",
    "    Evaluates predictions using quadratic weighted kappa.\n",
    "    \"\"\"\n",
    "    rounded_preds = threshold_rounder(oof_non_rounded, thresholds)\n",
    "    return -calculate_qwk(y_true, rounded_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values = X.isna()\n",
    "\n",
    "# Find columns with any null values\n",
    "columns_with_nulls = null_values.any()\n",
    "\n",
    "print(\"Columns with null values:\")\n",
    "print(columns_with_nulls[columns_with_nulls])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: lgb on Fold 1\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001976 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18237\n",
      "[LightGBM] [Info] Number of data points in the train set: 2188, number of used features: 119\n",
      "[LightGBM] [Info] Start training from score 0.582267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold:  20%|██        | 1/5 [00:01<00:07,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Train QWK: 0.9996, Validation QWK: 0.3924\n",
      "Training model: lgb on Fold 2\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002181 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18357\n",
      "[LightGBM] [Info] Number of data points in the train set: 2189, number of used features: 119\n",
      "[LightGBM] [Info] Start training from score 0.582915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold:  40%|████      | 2/5 [00:03<00:05,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - Train QWK: 0.9992, Validation QWK: 0.3927\n",
      "Training model: lgb on Fold 3\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001606 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18265\n",
      "[LightGBM] [Info] Number of data points in the train set: 2189, number of used features: 119\n",
      "[LightGBM] [Info] Start training from score 0.582001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold:  60%|██████    | 3/5 [00:05<00:03,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - Train QWK: 0.9992, Validation QWK: 0.4413\n",
      "Training model: lgb on Fold 4\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001574 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18184\n",
      "[LightGBM] [Info] Number of data points in the train set: 2189, number of used features: 119\n",
      "[LightGBM] [Info] Start training from score 0.582001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold:  80%|████████  | 4/5 [00:07<00:01,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 - Train QWK: 0.9992, Validation QWK: 0.3685\n",
      "Training model: lgb on Fold 5\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001666 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18482\n",
      "[LightGBM] [Info] Number of data points in the train set: 2189, number of used features: 119\n",
      "[LightGBM] [Info] Start training from score 0.582001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold: 100%|██████████| 5/5 [00:09<00:00,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 - Train QWK: 0.9992, Validation QWK: 0.4126\n",
      "Mean Train QWK --> 0.9993\n",
      "Mean Validation QWK ---> 0.4015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "\n",
    "# Don't want to one-hot encode the original datasets, so we make a copy\n",
    "X_reg, test_df_reg = categorical_encode_seasons(X, test_df)\n",
    "\n",
    "SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "models = [\n",
    "    # (\"rf\", RandomForestRegressor(n_estimators=200, max_depth=5, min_samples_split=2, random_state=42)),\n",
    "    (\"xgb\", XGBRegressor(n_estimators=200, max_depth=3, learning_rate=0.05, subsample=0.8, colsample_bytree=1, reg_alpha=0.5, reg_lambda=1, random_state=42)),\n",
    "    # (\"gb\", GradientBoostingRegressor(n_estimators=200, random_state=42)),\n",
    "    (\"lgb\", LGBMRegressor(num_leaves=33, subsample=0.6, reg_lambda=0.5, reg_alpha=0.5, min_child_samples=10, max_depth=-1, learning_rate=0.1, colsample_bytree=0.8, random_state=42)),   \n",
    "]\n",
    "\n",
    "train_S = []\n",
    "test_S = []\n",
    "\n",
    "oof_non_rounded = np.zeros(len(y), dtype=float)\n",
    "test_preds_per_model = np.zeros((len(test_df_reg), len(models), n_splits))\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X_reg, y), desc=\"Training Fold\", total=n_splits)):\n",
    "    X_train, X_val = X_reg.iloc[train_idx], X_reg.iloc[test_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    fold_val_preds = np.zeros((len(y_val), len(models)))\n",
    "\n",
    "    for model_idx, (name, model) in enumerate(models):\n",
    "        print(f\"Training model: {name} on Fold {fold + 1}\")\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        fold_val_preds[:, model_idx] = model.predict(X_val)\n",
    "\n",
    "        # Test predictions for this model\n",
    "        test_preds_per_model[:, model_idx, fold] = model.predict(test_df_reg)\n",
    "    \n",
    "    # Average predictions across models for validation\n",
    "    y_val_pred_ensemble = fold_val_preds.mean(axis=1)\n",
    "    oof_non_rounded[test_idx] = y_val_pred_ensemble\n",
    "\n",
    "    # Calculate metrics for each fold\n",
    "    train_kappa = calculate_qwk(y_train, model.predict(X_train).round(0).astype(int))\n",
    "    val_kappa = calculate_qwk(y_val, y_val_pred_ensemble.round(0).astype(int))\n",
    "\n",
    "\n",
    "    train_S.append(train_kappa)\n",
    "    test_S.append(val_kappa)\n",
    "    print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "\n",
    "final_test_preds = test_preds_per_model.mean(axis=2).mean(axis=1)\n",
    "\n",
    "\n",
    "initial_thresholds = np.quantile(oof_non_rounded, [0.25, 0.5, 0.75])\n",
    "optimizer = minimize(evaluate_predictions, x0=initial_thresholds, args=(y, oof_non_rounded), method='Nelder-Mead')\n",
    "thresholds = optimizer.x if optimizer.success else [0.5, 1.5, 2.5]\n",
    "\n",
    "\n",
    "print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "# print(\"Final Test Predictions (Averaged):\")\n",
    "# print(final_test_preds)\n",
    "\n",
    "# Best scores: \n",
    "# model = RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_split=2, random_state=42)\n",
    "#   Mean Train QWK --> 0.8003 | Mean Validation QWK ---> 0.3587\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_tuned = threshold_rounder(oof_non_rounded, thresholds)\n",
    "test_predictions = threshold_rounder(final_test_preds, thresholds)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df_ids.values,\n",
    "    'sii': test_predictions\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from lightgbm import LGBMRegressor\n",
    "# import numpy as np\n",
    "\n",
    "# # Define the base model\n",
    "# lgb_model = LGBMRegressor(n_estimators=200, random_state=42)\n",
    "\n",
    "# # Define the parameter distribution\n",
    "# param_distributions = {\n",
    "#     'num_leaves': [20,33,45],\n",
    "#     'max_depth': [-1, 10, 20],\n",
    "#     'learning_rate': [0.01, 0.05, 0.1],\n",
    "#     'min_child_samples': [10,20,30],\n",
    "#     'subsample': [0.6, 0.8, 1.0],\n",
    "#     'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "#     'reg_alpha': np.linspace(0, 1.0, 5),\n",
    "#     'reg_lambda': np.linspace(0, 2.0, 5)\n",
    "# }\n",
    "\n",
    "# # Create the RandomizedSearchCV object\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=lgb_model,\n",
    "#     param_distributions=param_distributions,\n",
    "#     n_iter=60,  # Number of parameter settings to sample\n",
    "#     scoring='neg_mean_squared_error',\n",
    "#     cv=5,\n",
    "#     verbose=1,\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# # Fit the randomized search\n",
    "# random_search.fit(X_reg, y)\n",
    "\n",
    "# # Print the best parameters and score\n",
    "# print(\"Best Parameters:\", random_search.best_params_)\n",
    "# print(\"Best Score:\", -random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Best Parameters:\", random_search.best_params_)\n",
    "# print(\"Best Score:\", -random_search.best_score_)\n",
    "\n",
    "# results = pd.DataFrame(random_search.cv_results_)\n",
    "\n",
    "# results['mean_test_score'] = -results['mean_test_score']\n",
    "# top_results = results.sort_values(by='mean_test_score', ascending=False).head(5)\n",
    "# top_results[['mean_test_score', 'params']].to_csv('asdfasdf.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_splits = 5\n",
    "\n",
    "# # Don't want to one-hot encode the original datasets, so we make a copy\n",
    "# X_reg, test_df_reg = categorical_encode_seasons(X, test_df)\n",
    "\n",
    "# SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# test_preds = np.zeros((len(test_df_reg), n_splits))\n",
    "# train_S = []\n",
    "# test_S = []\n",
    "\n",
    "# oof_non_rounded = np.zeros(len(y), dtype=float)\n",
    "\n",
    "# for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X_reg, y), desc=\"Training Fold\", total=n_splits)):\n",
    "#     X_train, X_val = X_reg.iloc[train_idx], X_reg.iloc[test_idx]\n",
    "#     y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "#     model = RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_split=2, random_state=42)\n",
    "#     model.fit(X_train, y_train)\n",
    "\n",
    "#     y_train_pred = model.predict(X_train)\n",
    "#     y_val_pred = model.predict(X_val)\n",
    "\n",
    "#     oof_non_rounded[test_idx] = y_val_pred\n",
    "\n",
    "#     train_kappa = calculate_qwk(y_train, y_train_pred.round(0).astype(int))\n",
    "#     val_kappa = calculate_qwk(y_val, y_val_pred.round(0).astype(int))\n",
    "    \n",
    "#     train_S.append(train_kappa)\n",
    "#     test_S.append(val_kappa)\n",
    "\n",
    "#     test_preds[:, fold] = model.predict(test_df_reg)\n",
    "#     print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "\n",
    "# print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "# print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "# initial_thresholds = np.quantile(oof_non_rounded, [0.25, 0.5, 0.75])\n",
    "# optimizer = minimize(evaluate_predictions, x0=initial_thresholds, args=(y, oof_non_rounded), method='Nelder-Mead')\n",
    "# thresholds = optimizer.x if optimizer.success else [0.5, 1.5, 2.5]\n",
    "\n",
    "# # Best scores: \n",
    "# # model = RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_split=2, random_state=42)\n",
    "# #   Mean Train QWK --> 0.8003 | Mean Validation QWK ---> 0.3587\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oof_tuned = threshold_rounder(oof_non_rounded, thresholds)\n",
    "# test_predictions = threshold_rounder(test_preds.mean(axis=1), thresholds)\n",
    "\n",
    "# submission = pd.DataFrame({\n",
    "#     'id': test_df_ids.values,\n",
    "#     'sii': test_predictions\n",
    "# })\n",
    "# submission.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Use this in the training loop\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.metrics import make_scorer, cohen_kappa_score\n",
    "\n",
    "# def qwk_scorer(y_true, y_pred):\n",
    "#     rounded_y_pred = threshold_rounder(y_pred, thresholds=[0.5, 1.5, 2.5])  # Adjust thresholds as needed\n",
    "#     return cohen_kappa_score(y_true, rounded_y_pred, weights='quadratic')\n",
    "\n",
    "# qwk_scorer = make_scorer(qwk_scorer, greater_is_better=True)  # Create a scorer object\n",
    "\n",
    "\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200, 300],\n",
    "#     'max_depth': [5, 10, 15],\n",
    "#     'min_samples_split': [2, 5, 10]\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=RandomForestRegressor(random_state=42),\n",
    "#     param_grid=param_grid,\n",
    "#     scoring=qwk_scorer,  # Use your desired scoring metric\n",
    "#     cv=5  # Use 5-fold cross-validation\n",
    "# )\n",
    "\n",
    "# grid_search.fit(X, y)\n",
    "\n",
    "# best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_score = grid_search.best_score_\n",
    "\n",
    "# # Get the cross-validation results\n",
    "# cv_results = grid_search.cv_results_\n",
    "\n",
    "# # Find the indices of the top 2 models\n",
    "# top_4_indices = np.argsort(cv_results['mean_test_score'])[-4:]  # Sort by mean test score and get the last two indices\n",
    "\n",
    "# # Get the scores and parameters of the top 2 models\n",
    "# for index in top_4_indices:\n",
    "#     score = cv_results['mean_test_score'][index]\n",
    "#     params = cv_results['params'][index]\n",
    "#     print(f\"Score: {score:.4f}, Params: {params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from catboost import CatBoostRegressor\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# SEED = 42\n",
    "# n_splits=5\n",
    "# train = X.copy()\n",
    "# # test = y.copy()\n",
    "\n",
    "# # season_cols = [col for col in train.columns if 'Season' in col]\n",
    "# # train = train.drop(season_cols, axis=1) \n",
    "\n",
    "# cat_c = ['Basic_Demos-Enroll_Season','CGAS-Season','Physical-Season','Fitness_Endurance-Season']\n",
    "# # FGC-Season: object, BIA-Season: object, PAQ_A-Season: object, PAQ_C-Season: object, PCIAT-Season: object, SDS-Season: object, PreInt_EduHx-Season: object\n",
    "\n",
    "# def update(df):\n",
    "#     global cat_c\n",
    "#     for c in cat_c: \n",
    "#         df[c] = df[c].fillna('Missing')\n",
    "#         df[c] = df[c].astype('category')\n",
    "#     return df\n",
    "        \n",
    "# train = update(train)\n",
    "# # test = update(test)\n",
    "\n",
    "# def create_mapping(column, dataset):\n",
    "#     unique_values = dataset[column].unique()\n",
    "#     return {value: idx for idx, value in enumerate(unique_values)}\n",
    "\n",
    "# for col in cat_c:\n",
    "#     mapping = create_mapping(col, train)\n",
    "#     # mappingTe = create_mapping(col, test)\n",
    "    \n",
    "#     train[col] = train[col].replace(mapping).astype(int)\n",
    "#     # test[col] = test[col].replace(mappingTe).astype(int)\n",
    "\n",
    "# # for i in train.columns:\n",
    "# #     if 'Season' in i:\n",
    "# #         print(i)\n",
    "# # # train['Basic_Demos-Enroll_Season']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = lgb.Dataset(X_full_train, label=y_full_train)\n",
    "\n",
    "# # Set parameters for multi-class classification\n",
    "# params = {\n",
    "#     'objective': 'multiclass',\n",
    "#     'metric': 'multi_logloss',\n",
    "#     'num_class': 4,  # Number of classes\n",
    "#     'boosting_type': 'gbdt',\n",
    "#     'num_leaves': 31,\n",
    "#     'learning_rate': 0.05,\n",
    "#     'feature_fraction': 0.9,\n",
    "# }\n",
    "\n",
    "# # Train the model\n",
    "# model = lgb.train(params, train_data, num_boost_round=100)\n",
    "\n",
    "# # Predict class probabilities for each class\n",
    "# y_pred_proba = model.predict(X_test)\n",
    "\n",
    "# # Convert probabilities to class labels (e.g., by choosing the class with the highest probability)\n",
    "# y_pred = y_pred_proba.argmax(axis=1)\n",
    "\n",
    "# qwk_score = cohen_kappa_score(y_test, y_pred, weights=\"quadratic\")\n",
    "# print(f\"Quadratic Weighted Kappa: {qwk_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 10)\n",
    "# possible_cat_features = ['FGC-FGC_CU_Zone','FGC-FGC_GSND_Zone','FGC-FGC_GSD_Zone','FGC-FGC_PU_Zone','FGC-FGC_SRL_Zone','FGC-FGC_SRR_Zone','FGC-FGC_TL_Zone','BIA-BIA_Activity_Level_num','BIA-BIA_Frame_num','PCIAT-PCIAT_01','PCIAT-PCIAT_02','PCIAT-PCIAT_03','PCIAT-PCIAT_04','PCIAT-PCIAT_05','PCIAT-PCIAT_06','PCIAT-PCIAT_07','PCIAT-PCIAT_08','PCIAT-PCIAT_09','PCIAT-PCIAT_10','PCIAT-PCIAT_11','PCIAT-PCIAT_12','PCIAT-PCIAT_13','PCIAT-PCIAT_14','PCIAT-PCIAT_15','PCIAT-PCIAT_16','PCIAT-PCIAT_17','PCIAT-PCIAT_18','PCIAT-PCIAT_19','PCIAT-PCIAT_20','PreInt_EduHx-computerinternet_hoursday']\n",
    "# cat_features = [col for col in X_full_train.columns if col in possible_cat_features]\n",
    "# catboost = CatBoostClassifier(iterations=1000,\n",
    "#                     depth=6,\n",
    "#                     learning_rate=0.05,\n",
    "#                     loss_function='MultiClass',\n",
    "#                     verbose=False,\n",
    "#                     l2_leaf_reg=4,\n",
    "#                     cat_features=cat_features\n",
    "#                     )\n",
    "\n",
    "\n",
    "# # lightgbm = LGBMClassifier(n_estimators=500, learning_rate=0.05, max_depth=6)\n",
    "# lightgbm = LGBMClassifier(n_estimators=500, learning_rate=0.05, max_depth=6, verbose=-1)\n",
    "\n",
    "# model = VotingClassifier(estimators=[('catboost', catboost), ('lightgbm', lightgbm)], voting='soft')\n",
    "\n",
    "# model.fit(X_full_train, y_full_train)\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_full_train, y_full_train)\n",
    "\n",
    "# # Make the prediction using the resulting model\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Calculate the Quadratic Weighted Kappa score\n",
    "# qwk_score = cohen_kappa_score(y_test, y_pred, weights=\"quadratic\")\n",
    "# print(f\"Quadratic Weighted Kappa: {qwk_score:.4f}\")\n",
    "# print(f\"\\nQuadratic Weighted Kappa: {calculate_qwk(y_test, y_pred):.4f}\\n\")\n",
    "# print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "# print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# # ids = X_full_train['id']\n",
    "# # y_pred.to_csv('training_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply model to test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids = test_df['id']\n",
    "# X_final = test_df.drop(columns=['id'] + [col for col in test_df.columns if 'PCIAT-PCIAT' in col])\n",
    "# y_pred_final = model.predict(X_final)\n",
    "# y_pred_final = y_pred_final.argmax(axis=1)\n",
    "\n",
    "# submission = pd.DataFrame({\n",
    "#     'id': ids,\n",
    "#     'prediction': y_pred_final\n",
    "# })\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "# submission\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
